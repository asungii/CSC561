{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOADYxK+TVs4YKR+Vbdv4+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asungii/CSC561/blob/main/SmoothQuant_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Installations</h1>\n",
        "\n",
        "From SmoothQuant repo. Because the specified CUDA compiler is 11.3, and the default Colab CUDA version is as of 7/29/24 12.2, we have to change the CUDA version in Colab.\n",
        "\n",
        "I found info on how to do this from:\n",
        "\n",
        "https://medium.com/@ajithkumarv/how-to-modify-cuda-gcc-python-versions-in-colab-584ed4113157\n",
        "\n",
        "https://github.com/googlecolab/colabtools/issues/4214#issuecomment-1859155789\n",
        "\n",
        "https://vitalitylearning.medium.com/running-cuda-in-google-colab-525a92efcf75\n",
        "\n",
        "This is going to be a retryâ€”I'm going to figure out if I can do this submodule work as intended so that this will work."
      ],
      "metadata": {
        "id": "nKLSnmrnyb2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Conda Setup</h2>"
      ],
      "metadata": {
        "id": "zOaorLmNLpkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "import os\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfS1kkONLoUt",
        "outputId": "59c56c87-8fca-450c-f3ad-8597286788b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:07\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yAECXIrL19X",
        "outputId": "1d10ba6f-a25b-4df4-c16c-44a8c5f40c67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 23.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: you can only use the base env in condacolab, so it's basically useless."
      ],
      "metadata": {
        "id": "O3gvi0ZYydWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K6NI2WxlYpUM",
        "outputId": "08e53dc1-fdac-402d-bc82-65254c3724f2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.7.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/smoothquant\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bzip2-1.0.8                |       h4bc722e_7         247 KB  conda-forge\n",
            "    ca-certificates-2024.7.4   |       hbcca054_0         151 KB  conda-forge\n",
            "    ld_impl_linux-64-2.40      |       hf3520f5_7         691 KB  conda-forge\n",
            "    libgcc-ng-14.1.0           |       h77fa898_0         822 KB  conda-forge\n",
            "    libgomp-14.1.0             |       h77fa898_0         446 KB  conda-forge\n",
            "    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n",
            "    libxcrypt-4.4.36           |       hd590300_1          98 KB  conda-forge\n",
            "    libzlib-1.3.1              |       h4ab18f5_1          60 KB  conda-forge\n",
            "    ncurses-6.5                |       h59595ed_0         867 KB  conda-forge\n",
            "    openssl-3.3.1              |       h4bc722e_2         2.8 MB  conda-forge\n",
            "    pip-24.0                   |     pyhd8ed1ab_0         1.3 MB  conda-forge\n",
            "    python-3.8.19              |hd12c33a_0_cpython        21.3 MB  conda-forge\n",
            "    setuptools-71.0.4          |     pyhd8ed1ab_0         1.4 MB  conda-forge\n",
            "    wheel-0.43.0               |     pyhd8ed1ab_1          57 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        31.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2024.7.4-hbcca054_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-hf3520f5_7 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.1.0-h77fa898_0 \n",
            "  libgomp            conda-forge/linux-64::libgomp-14.1.0-h77fa898_0 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.46.0-hde9e2c9_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-h4ab18f5_1 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h59595ed_0 \n",
            "  openssl            conda-forge/linux-64::openssl-3.3.1-h4bc722e_2 \n",
            "  pip                conda-forge/noarch::pip-24.0-pyhd8ed1ab_0 \n",
            "  python             conda-forge/linux-64::python-3.8.19-hd12c33a_0_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n",
            "  setuptools         conda-forge/noarch::setuptools-71.0.4-pyhd8ed1ab_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  wheel              conda-forge/noarch::wheel-0.43.0-pyhd8ed1ab_1 \n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.8.19        | 21.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "openssl-3.3.1        | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "setuptools-71.0.4    | 1.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pip-24.0             | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.19        | 21.3 MB   | :   3% 0.027847613894894437/1 [00:00<00:03,  3.60s/it]\n",
            "openssl-3.3.1        | 2.8 MB    | :  38% 0.3848117565097974/1 [00:00<00:00,  3.84it/s]\u001b[A\n",
            "\n",
            "\n",
            "pip-24.0             | 1.3 MB    | :  70% 0.7030527554183995/1 [00:00<00:00,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   2% 0.018933467075597506/1 [00:00<00:07,  7.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | :   2% 0.019455913664383113/1 [00:00<00:07,  7.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.19        | 21.3 MB   | :  18% 0.18100949031681385/1 [00:00<00:00,  1.01it/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | :   4% 0.035857088143568416/1 [00:00<00:06,  6.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   6% 0.06481448515129577/1 [00:00<00:03,  3.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.19        | 21.3 MB   | :  32% 0.323178887569696/1 [00:00<00:00,  1.20it/s]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :  16% 0.163198629386511/1 [00:00<00:01,  1.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :  27% 0.2660863351414558/1 [00:00<00:00,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.8.19        | 21.3 MB   | :  95% 0.9512158640940258/1 [00:00<00:00,  1.93it/s]\n",
            "\n",
            "\n",
            "pip-24.0             | 1.3 MB    | : 100% 1.0/1 [00:01<00:00,  7.02it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:01<00:00,  1.09s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:01<00:00,  1.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "setuptools-71.0.4    | 1.4 MB    | : 100% 1.0/1 [00:01<00:00,  1.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "setuptools-71.0.4    | 1.4 MB    | : 100% 1.0/1 [00:01<00:00,  1.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | : 100% 1.0/1 [00:01<00:00,  1.24s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | : 100% 1.0/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | : 100% 1.0/1 [00:01<00:00,  1.28s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | : 100% 1.0/1 [00:01<00:00,  1.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.47s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | : 100% 1.0/1 [00:01<00:00,  1.52s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | : 100% 1.0/1 [00:01<00:00,  1.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:01<00:00,  1.58s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:01<00:00,  1.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:01<00:00,  1.66s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:01<00:00,  1.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.43.0         | 57 KB     | : 100% 1.0/1 [00:01<00:00,  1.73s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.43.0         | 57 KB     | : 100% 1.0/1 [00:01<00:00,  1.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.3.1        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.88s/it]               \u001b[A\n",
            "openssl-3.3.1        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.88s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | : 100% 1.0/1 [00:02<00:00,  2.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate smoothquant\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "\n",
            "CondaError: Run 'conda init' before 'conda activate'\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch==1.12.1+cu113)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy (from torchvision==0.13.1+cu113)\n",
            "  Downloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.13.1+cu113)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.13.1+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.13.1+cu113) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.13.1+cu113) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torchvision==0.13.1+cu113) (2023.11.17)\n",
            "Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: typing-extensions, pillow, numpy, torch, torchvision, torchaudio\n",
            "Successfully installed numpy-2.0.1 pillow-10.4.0 torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113 typing-extensions-4.12.2\n",
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/site-packages (0.22.0)\n",
            "Collecting filelock (from transformers==4.36.0)\n",
            "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.36.0)\n",
            "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.36.0) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.36.0) (23.2)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.36.0)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.36.0)\n",
            "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.36.0) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers==4.36.0)\n",
            "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.36.0) (4.66.1)\n",
            "Collecting numpy>=1.17 (from transformers==4.36.0)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m161.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from accelerate)\n",
            "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (1.12.1+cu113)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting requests (from transformers==4.36.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm>=4.27 (from transformers==4.36.0)\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.36.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.36.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.36.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.36.0) (2023.11.17)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->datasets)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.24.3-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.3/417.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, tqdm, six, safetensors, requests, regex, pyyaml, pyarrow-hotfix, psutil, numpy, multidict, fsspec, frozenlist, filelock, dill, attrs, async-timeout, yarl, python-dateutil, pyarrow, multiprocess, huggingface-hub, aiosignal, tokenizers, pandas, aiohttp, accelerate, transformers, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.1\n",
            "    Uninstalling numpy-2.0.1:\n",
            "      Successfully uninstalled numpy-2.0.1\n",
            "Successfully installed accelerate-0.33.0 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.20.0 dill-0.3.8 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.24.3 multidict-6.0.5 multiprocess-0.70.16 numpy-1.26.4 pandas-2.2.2 psutil-6.0.0 pyarrow-17.0.0 pyarrow-hotfix-0.6 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0.1 regex-2024.7.24 requests-2.32.3 safetensors-0.4.3 six-1.16.0 tokenizers-0.15.2 tqdm-4.66.4 transformers-4.36.0 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              },
              "id": "912f91ec85b2402383731c59419a0149"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# setup is from the mit-han-lab GitHub\n",
        "\n",
        "# these bits are not necessary for Google Colab. conda is getting on my nerves\n",
        "# !conda create -n smoothquant python=3.8\n",
        "# !conda activate smoothquant\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install transformers==4.36.0 accelerate datasets zstandard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/smoothquant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwpnccEtNLSP",
        "outputId": "4f3da336-815b-45e0-ae93-18872848e735"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'smoothquant'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 352 (delta 130), reused 117 (delta 101), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (352/352), 6.80 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/smoothquant\n",
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u4grsIO0NZVg",
        "outputId": "9d7447be-19a6-4a1d-d73f-bd6cbf8d5887"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/smoothquant\n",
            "running install\n",
            "/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating smoothquant.egg-info\n",
            "writing smoothquant.egg-info/PKG-INFO\n",
            "writing dependency_links to smoothquant.egg-info/dependency_links.txt\n",
            "writing top-level names to smoothquant.egg-info/top_level.txt\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "reading manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'smoothquant.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/smoothquant\n",
            "copying smoothquant/calibration.py -> build/lib/smoothquant\n",
            "copying smoothquant/ppl_eval.py -> build/lib/smoothquant\n",
            "copying smoothquant/fake_quant.py -> build/lib/smoothquant\n",
            "copying smoothquant/opt.py -> build/lib/smoothquant\n",
            "copying smoothquant/smooth.py -> build/lib/smoothquant\n",
            "copying smoothquant/__init__.py -> build/lib/smoothquant\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/calibration.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/ppl_eval.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/fake_quant.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/opt.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/smooth.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "copying build/lib/smoothquant/__init__.py -> build/bdist.linux-x86_64/egg/smoothquant\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/calibration.py to calibration.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/ppl_eval.py to ppl_eval.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/fake_quant.py to fake_quant.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/opt.py to opt.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/smooth.py to smooth.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/smoothquant/__init__.py to __init__.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying smoothquant.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/smoothquant-0.0.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing smoothquant-0.0.0-py3.10.egg\n",
            "Copying smoothquant-0.0.0-py3.10.egg to /usr/local/lib/python3.10/site-packages\n",
            "Adding smoothquant 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/site-packages/smoothquant-0.0.0-py3.10.egg\n",
            "Processing dependencies for smoothquant==0.0.0\n",
            "Finished processing dependencies for smoothquant==0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hn47j_Z9un1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "45Rh36UA8efT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version\n",
        "\n",
        "# well it's 12.2. that's a bit of an issue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azoig1mPz7bV",
        "outputId": "2456465b-41f0-4ef6-a1ea-92fa687b80ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
        "!sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.3\n",
        "\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrTVEc6byXAs",
        "outputId": "1b331a21-fce6-4122-e04e-b741d64e9741"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-29 22:54:12--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190 [application/octet-stream]\n",
            "Saving to: â€˜cuda-ubuntu2004.pinâ€™\n",
            "\n",
            "\rcuda-ubuntu2004.pin   0%[                    ]       0  --.-KB/s               \rcuda-ubuntu2004.pin 100%[===================>]     190  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-29 22:54:12 (5.75 MB/s) - â€˜cuda-ubuntu2004.pinâ€™ saved [190/190]\n",
            "\n",
            "--2024-07-29 22:54:12--  https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2286573544 (2.1G) [application/x-deb]\n",
            "Saving to: â€˜cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.debâ€™\n",
            "\n",
            "cuda-repo-ubuntu200 100%[===================>]   2.13G   351MB/s    in 7.4s    \n",
            "\n",
            "2024-07-29 22:54:20 (294 MB/s) - â€˜cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.debâ€™ saved [2286573544/2286573544]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu2004-11-3-local.\n",
            "(Reading database ... 123589 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu2004-11-3-local_11.3.0-465.19.01-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "Setting up cuda-repo-ubuntu2004-11-3-local (11.3.0-465.19.01-1) ...\n",
            "\n",
            "The public CUDA GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Get:3 file:/var/cuda-repo-ubuntu2004-11-3-local  Release.gpg [836 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 file:/var/cuda-repo-ubuntu2004-11-3-local  Packages [30.3 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,130 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,877 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,208 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,112 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,780 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,390 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Get:24 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,549 kB]\n",
            "Fetched 23.9 MB in 5s (5,204 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-11.3-1' for regex 'cuda-11.3'\n",
            "Note, selecting 'cuda-11-3' for regex 'cuda-11.3'\n",
            "The following additional packages will be installed:\n",
            "  cpp-12 cuda-command-line-tools-11-3 cuda-compiler-11-3 cuda-cudart-11-3\n",
            "  cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3 cuda-cupti-dev-11-3\n",
            "  cuda-cuxxfilt-11-3 cuda-demo-suite-11-3 cuda-documentation-11-3\n",
            "  cuda-driver-dev-11-3 cuda-drivers cuda-drivers-555 cuda-gdb-11-3\n",
            "  cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-555 libnvidia-common-555\n",
            "  libnvidia-compute-555 libnvidia-decode-555 libnvidia-encode-555\n",
            "  libnvidia-extra-555 libnvidia-fbc1-555 libnvidia-gl-555 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libudev1 libxcvt0 libxfont2\n",
            "  libxkbfile1 libxtst6 libxxf86dga1 nsight-compute-2021.1.0\n",
            "  nsight-systems-2021.1.3 nvidia-compute-utils-555 nvidia-dkms-555\n",
            "  nvidia-driver-555 nvidia-firmware-555-555.42.06 nvidia-kernel-common-555\n",
            "  nvidia-kernel-source-555 nvidia-prime nvidia-settings nvidia-utils-555\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-555\n",
            "Suggested packages:\n",
            "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc gvfs\n",
            "  mesa-utils xfs | xserver xfonts-100dpi | xfonts-75dpi xfonts-scalable\n",
            "Recommended packages:\n",
            "  libnvidia-compute-555:i386 libnvidia-decode-555:i386\n",
            "  libnvidia-encode-555:i386 libnvidia-fbc1-555:i386 libnvidia-gl-555:i386\n",
            "The following NEW packages will be installed:\n",
            "  cpp-12 cuda-11-3 cuda-command-line-tools-11-3 cuda-compiler-11-3\n",
            "  cuda-cudart-11-3 cuda-cudart-dev-11-3 cuda-cuobjdump-11-3 cuda-cupti-11-3\n",
            "  cuda-cupti-dev-11-3 cuda-cuxxfilt-11-3 cuda-demo-suite-11-3\n",
            "  cuda-documentation-11-3 cuda-driver-dev-11-3 cuda-drivers cuda-drivers-555\n",
            "  cuda-gdb-11-3 cuda-libraries-11-3 cuda-libraries-dev-11-3 cuda-memcheck-11-3\n",
            "  cuda-nsight-11-3 cuda-nsight-compute-11-3 cuda-nsight-systems-11-3\n",
            "  cuda-nvcc-11-3 cuda-nvdisasm-11-3 cuda-nvml-dev-11-3 cuda-nvprof-11-3\n",
            "  cuda-nvprune-11-3 cuda-nvrtc-11-3 cuda-nvrtc-dev-11-3 cuda-nvtx-11-3\n",
            "  cuda-nvvp-11-3 cuda-runtime-11-3 cuda-samples-11-3 cuda-sanitizer-11-3\n",
            "  cuda-thrust-11-3 cuda-toolkit-11-3 cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-3 cuda-visual-tools-11-3 dctrl-tools default-jre\n",
            "  default-jre-headless dkms fakeroot fonts-dejavu-core fonts-dejavu-extra\n",
            "  gcc-12 keyboard-configuration libasan8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-3 libcublas-dev-11-3 libcufft-11-3\n",
            "  libcufft-dev-11-3 libcurand-11-3 libcurand-dev-11-3 libcusolver-11-3\n",
            "  libcusolver-dev-11-3 libcusparse-11-3 libcusparse-dev-11-3 libfakeroot\n",
            "  libfontenc1 libgail-common libgail18 libgcc-12-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libjansson4 liblocale-gettext-perl libnpp-11-3\n",
            "  libnpp-dev-11-3 libnvidia-cfg1-555 libnvidia-common-555\n",
            "  libnvidia-compute-555 libnvidia-decode-555 libnvidia-encode-555\n",
            "  libnvidia-extra-555 libnvidia-fbc1-555 libnvidia-gl-555 libnvjpeg-11-3\n",
            "  libnvjpeg-dev-11-3 librsvg2-common libtsan2 libxcvt0 libxfont2 libxkbfile1\n",
            "  libxtst6 libxxf86dga1 nsight-compute-2021.1.0 nsight-systems-2021.1.3\n",
            "  nvidia-compute-utils-555 nvidia-dkms-555 nvidia-driver-555\n",
            "  nvidia-firmware-555-555.42.06 nvidia-kernel-common-555\n",
            "  nvidia-kernel-source-555 nvidia-prime nvidia-settings nvidia-utils-555\n",
            "  openjdk-11-jre python3-xkit screen-resolution-extra systemd-hwe-hwdb udev\n",
            "  x11-utils x11-xkb-utils xcvt xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xserver-xorg-core xserver-xorg-video-nvidia-555\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 115 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 325 MB/2,361 MB of archives.\n",
            "After this operation, 5,413 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-11-3 11.3.58-1 [156 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-config-common 11.8.89-1 [16.4 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-555 555.42.06-0ubuntu1 [17.2 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-555 555.42.06-0ubuntu1 [46.9 MB]\n",
            "Get:5 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-11-3 11.3.58-1 [25.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-555 555.42.06-0ubuntu1 [138 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
            "Get:9 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-11-3 11.4.2.10064-1 [134 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-555 555.42.06-0ubuntu1 [41.4 MB]\n",
            "Get:12 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-11-3 10.4.2.58-1 [107 MB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-555-555.42.06 555.42.06-0ubuntu1 [36.5 MB]\n",
            "Get:14 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-11-3 10.2.4.58-1 [40.0 MB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-555 555.42.06-0ubuntu1 [109 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-555 555.42.06-0ubuntu1 [36.2 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-555 555.42.06-0ubuntu1 [73.4 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-555 555.42.06-0ubuntu1 [118 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-555 555.42.06-0ubuntu1 [1,792 kB]\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-555 555.42.06-0ubuntu1 [104 kB]\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-555 555.42.06-0ubuntu1 [496 kB]\n",
            "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-555 555.42.06-0ubuntu1 [146 kB]\n",
            "Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-555 555.42.06-0ubuntu1 [1,534 kB]\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-555 555.42.06-0ubuntu1 [75.8 kB]\n",
            "Get:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-555 555.42.06-0ubuntu1 [490 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-555 555.42.06-1 [2,546 B]\n",
            "Get:27 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 555.42.06-1 [2,506 B]\n",
            "Get:28 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 555.42.06-0ubuntu1 [946 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
            "Get:30 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-11-3 11.1.1.58-1 [75.7 MB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
            "Get:34 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-11-3 11.5.0.58-1 [99.1 MB]\n",
            "Get:35 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-11-3 11.3.3.44-1 [72.3 MB]\n",
            "Get:36 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-11-3 11.4.1.58-1 [1,736 kB]\n",
            "Get:37 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-11-3 11.3.0-1 [2,502 B]\n",
            "Get:38 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-runtime-11-3 11.3.0-1 [2,420 B]\n",
            "Get:39 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuobjdump-11-3 11.3.58-1 [112 kB]\n",
            "Get:40 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cuxxfilt-11-3 11.3.58-1 [44.1 kB]\n",
            "Get:41 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-thrust-11-3 11.3.58-1 [982 kB]\n",
            "Get:42 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-driver-dev-11-3 11.3.58-1 [26.2 kB]\n",
            "Get:43 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cudart-dev-11-3 11.3.58-1 [737 kB]\n",
            "Get:44 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvcc-11-3 11.3.58-1 [45.7 MB]\n",
            "Get:45 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprune-11-3 11.3.58-1 [54.9 kB]\n",
            "Get:46 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-compiler-11-3 11.3.0-1 [2,428 B]\n",
            "Get:47 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvrtc-dev-11-3 11.3.58-1 [23.3 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.11 [1,477 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.23+9-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:68 file:/var/cuda-repo-ubuntu2004-11-3-local  libcublas-dev-11-3 11.4.2.10064-1 [141 MB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/main amd64 nvidia-prime all 0.8.17.1 [9,956 B]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:87 file:/var/cuda-repo-ubuntu2004-11-3-local  libcufft-dev-11-3 10.4.2.58-1 [179 MB]\n",
            "Get:88 file:/var/cuda-repo-ubuntu2004-11-3-local  libcurand-dev-11-3 10.2.4.58-1 [40.4 MB]\n",
            "Get:89 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusolver-dev-11-3 11.1.1.58-1 [21.5 MB]\n",
            "Get:90 file:/var/cuda-repo-ubuntu2004-11-3-local  libcusparse-dev-11-3 11.5.0.58-1 [99.8 MB]\n",
            "Get:91 file:/var/cuda-repo-ubuntu2004-11-3-local  libnpp-dev-11-3 11.3.3.44-1 [70.4 MB]\n",
            "Get:92 file:/var/cuda-repo-ubuntu2004-11-3-local  libnvjpeg-dev-11-3 11.4.1.58-1 [1,428 kB]\n",
            "Get:93 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-libraries-dev-11-3 11.3.0-1 [2,530 B]\n",
            "Get:94 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-11-3 11.3.58-1 [11.5 MB]\n",
            "Get:95 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-cupti-dev-11-3 11.3.58-1 [2,401 kB]\n",
            "Get:96 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvdisasm-11-3 11.3.58-1 [32.9 MB]\n",
            "Get:97 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-gdb-11-3 11.3.58-1 [3,622 kB]\n",
            "Get:98 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-memcheck-11-3 11.3.58-1 [145 kB]\n",
            "Get:99 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvprof-11-3 11.3.58-1 [1,930 kB]\n",
            "Get:100 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvtx-11-3 11.3.58-1 [51.1 kB]\n",
            "Get:101 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-sanitizer-11-3 11.3.58-1 [7,534 kB]\n",
            "Get:102 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-command-line-tools-11-3 11.3.0-1 [2,466 B]\n",
            "Get:103 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-compute-2021.1.0 2021.1.0.18-1 [274 MB]\n",
            "Get:104 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-compute-11-3 11.3.0-1 [3,708 B]\n",
            "Get:105 file:/var/cuda-repo-ubuntu2004-11-3-local  nsight-systems-2021.1.3 2021.1.3.14-b695ea9 [248 MB]\n",
            "Get:106 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-systems-11-3 11.3.0-1 [3,302 B]\n",
            "Get:107 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nsight-11-3 11.3.58-1 [119 MB]\n",
            "Get:108 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvml-dev-11-3 11.3.58-1 [73.3 kB]\n",
            "Get:109 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-nvvp-11-3 11.3.58-1 [115 MB]\n",
            "Get:110 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-visual-tools-11-3 11.3.0-1 [2,868 B]\n",
            "Get:111 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-tools-11-3 11.3.0-1 [2,380 B]\n",
            "Get:112 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-samples-11-3 11.3.58-1 [59.2 MB]\n",
            "Get:113 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-documentation-11-3 11.3.58-1 [48.2 kB]\n",
            "Get:114 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-toolkit-11-3 11.3.0-1 [3,274 B]\n",
            "Get:115 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-demo-suite-11-3 11.3.58-1 [3,978 kB]\n",
            "Get:116 file:/var/cuda-repo-ubuntu2004-11-3-local  cuda-11-3 11.3.0-1 [2,450 B]\n",
            "Fetched 325 MB in 16s (20.0 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 116.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package liblocale-gettext-perl.\n",
            "(Reading database ... 123701 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
            "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
            "Selecting previously unselected package keyboard-configuration.\n",
            "Preparing to unpack .../1-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
            "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
            "Selecting previously unselected package cpp-12.\n",
            "Preparing to unpack .../2-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan8:amd64.\n",
            "Preparing to unpack .../3-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libtsan2:amd64.\n",
            "Preparing to unpack .../4-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-12-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-12.\n",
            "Preparing to unpack .../6-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package dctrl-tools.\n",
            "Preparing to unpack .../7-dctrl-tools_2.24-3build2_amd64.deb ...\n",
            "Unpacking dctrl-tools (2.24-3build2) ...\n",
            "Selecting previously unselected package dkms.\n",
            "Preparing to unpack .../8-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
            "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
            "Preparing to unpack .../9-libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 124061 files and directories currently installed.)\n",
            "Preparing to unpack .../000-udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "Preparing to unpack .../001-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-config-common.\n",
            "Preparing to unpack .../002-cuda-toolkit-11-config-common_11.8.89-1_all.deb ...\n",
            "Unpacking cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-11-3.\n",
            "Preparing to unpack .../003-cuda-cudart-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-3.\n",
            "Preparing to unpack .../004-cuda-nvrtc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-11-3.\n",
            "Preparing to unpack .../005-libcublas-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-11-3.\n",
            "Preparing to unpack .../006-libcufft-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-11-3.\n",
            "Preparing to unpack .../007-libcurand-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-11-3.\n",
            "Preparing to unpack .../008-libcusolver-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-11-3.\n",
            "Preparing to unpack .../009-libcusparse-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-11-3.\n",
            "Preparing to unpack .../010-libnpp-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-3.\n",
            "Preparing to unpack .../011-libnvjpeg-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-3.\n",
            "Preparing to unpack .../012-cuda-libraries-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libnvidia-common-555.\n",
            "Preparing to unpack .../013-libnvidia-common-555_555.42.06-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-555:amd64.\n",
            "Preparing to unpack .../014-libnvidia-compute-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-gl-555:amd64.\n",
            "Preparing to unpack .../015-libnvidia-gl-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-535\n",
            "Unpacking libnvidia-gl-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-source-555.\n",
            "Preparing to unpack .../016-nvidia-kernel-source-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-firmware-555-555.42.06.\n",
            "Preparing to unpack .../017-nvidia-firmware-555-555.42.06_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-firmware-555-555.42.06 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-common-555.\n",
            "Preparing to unpack .../018-nvidia-kernel-common-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-dkms-555.\n",
            "Preparing to unpack .../019-nvidia-dkms-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-dkms-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-extra-555:amd64.\n",
            "Preparing to unpack .../020-libnvidia-extra-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-extra-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-555.\n",
            "Preparing to unpack .../021-nvidia-compute-utils-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-decode-555:amd64.\n",
            "Preparing to unpack .../022-libnvidia-decode-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-555:amd64.\n",
            "Preparing to unpack .../023-libnvidia-encode-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-555.\n",
            "Preparing to unpack .../024-nvidia-utils-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-cfg1-555:amd64.\n",
            "Preparing to unpack .../025-libnvidia-cfg1-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../026-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../027-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../028-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package libxcvt0:amd64.\n",
            "Preparing to unpack .../029-libxcvt0_0.1.1-3_amd64.deb ...\n",
            "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../030-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../031-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package xserver-xorg-core.\n",
            "Preparing to unpack .../032-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xserver-xorg-video-nvidia-555.\n",
            "Preparing to unpack .../033-xserver-xorg-video-nvidia-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-fbc1-555:amd64.\n",
            "Preparing to unpack .../034-libnvidia-fbc1-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-555.\n",
            "Preparing to unpack .../035-nvidia-driver-555_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-555 (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package cuda-drivers-555.\n",
            "Preparing to unpack .../036-cuda-drivers-555_555.42.06-1_amd64.deb ...\n",
            "Unpacking cuda-drivers-555 (555.42.06-1) ...\n",
            "Selecting previously unselected package cuda-drivers.\n",
            "Preparing to unpack .../037-cuda-drivers_555.42.06-1_amd64.deb ...\n",
            "Unpacking cuda-drivers (555.42.06-1) ...\n",
            "Selecting previously unselected package cuda-runtime-11-3.\n",
            "Preparing to unpack .../038-cuda-runtime-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-3.\n",
            "Preparing to unpack .../039-cuda-cuobjdump-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-11-3.\n",
            "Preparing to unpack .../040-cuda-cuxxfilt-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-thrust-11-3.\n",
            "Preparing to unpack .../041-cuda-thrust-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-3.\n",
            "Preparing to unpack .../042-cuda-driver-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-3.\n",
            "Preparing to unpack .../043-cuda-cudart-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-3.\n",
            "Preparing to unpack .../044-cuda-nvcc-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-3.\n",
            "Preparing to unpack .../045-cuda-nvprune-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-3.\n",
            "Preparing to unpack .../046-cuda-compiler-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-3.\n",
            "Preparing to unpack .../047-cuda-nvrtc-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-3.\n",
            "Preparing to unpack .../048-libcublas-dev-11-3_11.4.2.10064-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-3.\n",
            "Preparing to unpack .../049-libcufft-dev-11-3_10.4.2.58-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-3.\n",
            "Preparing to unpack .../050-libcurand-dev-11-3_10.2.4.58-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-3.\n",
            "Preparing to unpack .../051-libcusolver-dev-11-3_11.1.1.58-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-3.\n",
            "Preparing to unpack .../052-libcusparse-dev-11-3_11.5.0.58-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-3.\n",
            "Preparing to unpack .../053-libnpp-dev-11-3_11.3.3.44-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-3.\n",
            "Preparing to unpack .../054-libnvjpeg-dev-11-3_11.4.1.58-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-3.\n",
            "Preparing to unpack .../055-cuda-libraries-dev-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-3.\n",
            "Preparing to unpack .../056-cuda-cupti-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-3.\n",
            "Preparing to unpack .../057-cuda-cupti-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-3.\n",
            "Preparing to unpack .../058-cuda-nvdisasm-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-3.\n",
            "Preparing to unpack .../059-cuda-gdb-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-3.\n",
            "Preparing to unpack .../060-cuda-memcheck-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-3.\n",
            "Preparing to unpack .../061-cuda-nvprof-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-3.\n",
            "Preparing to unpack .../062-cuda-nvtx-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-3.\n",
            "Preparing to unpack .../063-cuda-sanitizer-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-3.\n",
            "Preparing to unpack .../064-cuda-command-line-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-compute-2021.1.0.\n",
            "Preparing to unpack .../065-nsight-compute-2021.1.0_2021.1.0.18-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-3.\n",
            "Preparing to unpack .../066-cuda-nsight-compute-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package nsight-systems-2021.1.3.\n",
            "Preparing to unpack .../067-nsight-systems-2021.1.3_2021.1.3.14-1_amd64.deb ...\n",
            "Unpacking nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-3.\n",
            "Preparing to unpack .../068-cuda-nsight-systems-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../069-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../070-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../071-openjdk-11-jre_11.0.23+9-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.23+9-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../072-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-11-3.\n",
            "Preparing to unpack .../073-cuda-nsight-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-3.\n",
            "Preparing to unpack .../074-cuda-nvml-dev-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-3.\n",
            "Preparing to unpack .../075-cuda-nvvp-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-3.\n",
            "Preparing to unpack .../076-cuda-visual-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-3.\n",
            "Preparing to unpack .../077-cuda-tools-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-samples-11-3.\n",
            "Preparing to unpack .../078-cuda-samples-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-samples-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-3.\n",
            "Preparing to unpack .../079-cuda-documentation-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-3.\n",
            "Preparing to unpack .../080-cuda-toolkit-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-11-3.\n",
            "Preparing to unpack .../081-cuda-demo-suite-11-3_11.3.58-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Selecting previously unselected package cuda-11-3.\n",
            "Preparing to unpack .../082-cuda-11-3_11.3.0-1_amd64.deb ...\n",
            "Unpacking cuda-11-3 (11.3.0-1) ...\n",
            "Selecting previously unselected package libfakeroot:amd64.\n",
            "Preparing to unpack .../083-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fakeroot.\n",
            "Preparing to unpack .../084-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../085-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../086-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../087-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../088-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../089-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../090-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../091-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../092-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../093-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../094-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../095-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../096-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package nvidia-prime.\n",
            "Preparing to unpack .../097-nvidia-prime_0.8.17.1_all.deb ...\n",
            "Unpacking nvidia-prime (0.8.17.1) ...\n",
            "Selecting previously unselected package python3-xkit.\n",
            "Preparing to unpack .../098-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
            "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
            "Selecting previously unselected package screen-resolution-extra.\n",
            "Preparing to unpack .../099-screen-resolution-extra_0.18.2_all.deb ...\n",
            "Unpacking screen-resolution-extra (0.18.2) ...\n",
            "Selecting previously unselected package nvidia-settings.\n",
            "Preparing to unpack .../100-nvidia-settings_555.42.06-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-settings (555.42.06-0ubuntu1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../101-systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Selecting previously unselected package xcvt.\n",
            "Preparing to unpack .../102-xcvt_0.1.1-3_amd64.deb ...\n",
            "Unpacking xcvt (0.1.1-3) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../103-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../104-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../105-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Setting up libcublas-11-3 (11.4.2.10064-1) ...\n",
            "Setting up libnvidia-common-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-cuxxfilt-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-fbc1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libcublas-dev-11-3 (11.4.2.10064-1) ...\n",
            "Setting up cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Setting up cuda-nvtx-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-memcheck-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-cfg1-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-prime (0.8.17.1) ...\n",
            "Setting up cuda-nvprune-11-3 (11.3.58-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-driver-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.23+9-1ubuntu1~22.04.1) ...\n",
            "Setting up libnvjpeg-11-3 (11.4.1.58-1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Setting up cuda-nvprof-11-3 (11.3.58-1) ...\n",
            "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Setting up libnvidia-extra-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up fakeroot (1.28-1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
            "Setting up cuda-thrust-11-3 (11.3.58-1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up cuda-nvml-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-compute-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-firmware-555-555.42.06 (555.42.06-0ubuntu1) ...\n",
            "Setting up libnvjpeg-dev-11-3 (11.4.1.58-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up cuda-cudart-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-cudart-dev-11-3 (11.3.58-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libnpp-11-3 (11.3.3.44-1) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up nsight-compute-2021.1.0 (2021.1.0.18-1) ...\n",
            "Setting up libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcusparse-11-3 (11.5.0.58-1) ...\n",
            "Setting up nsight-systems-2021.1.3 (2021.1.3.14-b695ea9) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: error: alternative path /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsight-sys doesn't exist\n",
            "update-alternatives: error: no alternatives for nsight-sys\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2021.1.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up cuda-nvdisasm-11-3 (11.3.58-1) ...\n",
            "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
            "Setting up cuda-nvvp-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libcurand-11-3 (10.2.4.58-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cuobjdump-11-3 (11.3.58-1) ...\n",
            "Setting up libcufft-11-3 (10.4.2.58-1) ...\n",
            "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
            "Setting up libcusparse-dev-11-3 (11.5.0.58-1) ...\n",
            "Setting up cuda-nvrtc-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-sanitizer-11-3 (11.3.58-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
            "Setting up dctrl-tools (2.24-3build2) ...\n",
            "Setting up libcusolver-11-3 (11.1.1.58-1) ...\n",
            "Setting up nvidia-kernel-source-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up nvidia-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-nsight-systems-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-3 (11.3.58-1) ...\n",
            "Setting up libcurand-dev-11-3 (10.2.4.58-1) ...\n",
            "Setting up nvidia-compute-utils-555 (555.42.06-0ubuntu1) ...\n",
            "Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory\n",
            "Adding system user `nvidia-persistenced' (UID 104) ...\n",
            "Adding new group `nvidia-persistenced' (GID 111) ...\n",
            "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
            "Not creating home directory `/nonexistent'.\n",
            "Setting up cuda-nsight-compute-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-nvcc-11-3 (11.3.58-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up cuda-nsight-11-3 (11.3.58-1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnpp-dev-11-3 (11.3.3.44-1) ...\n",
            "Setting up cuda-libraries-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-gdb-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-decode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up nvidia-kernel-common-555 (555.42.06-0ubuntu1) ...\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-hibernate.service â†’ /lib/systemd/system/nvidia-hibernate.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-resume.service â†’ /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-hibernate.service.wants/nvidia-resume.service â†’ /lib/systemd/system/nvidia-resume.service.\n",
            "Created symlink /etc/systemd/system/systemd-suspend.service.wants/nvidia-suspend.service â†’ /lib/systemd/system/nvidia-suspend.service.\n",
            "Setting up cuda-compiler-11-3 (11.3.0-1) ...\n",
            "Setting up xcvt (0.1.1-3) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up libnvidia-gl-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up libcufft-dev-11-3 (10.4.2.58-1) ...\n",
            "Setting up screen-resolution-extra (0.18.2) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up nvidia-settings (555.42.06-0ubuntu1) ...\n",
            "Setting up libcusolver-dev-11-3 (11.1.1.58-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Configuring keyboard-configuration\n",
            "----------------------------------\n",
            "\n",
            "The layout of keyboards varies per country, with some countries having multiple\n",
            "common layouts. Please select the country of origin for the keyboard of this\n",
            "computer.\n",
            "\n",
            "  1. Afghani\n",
            "  2. Albanian\n",
            "  3. Amharic\n",
            "  4. Arabic\n",
            "  5. Arabic (Morocco)\n",
            "  6. Arabic (Syria)\n",
            "  7. Armenian\n",
            "  8. A user-defined custom Layout\n",
            "  9. Azerbaijani\n",
            "  10. Bambara\n",
            "  11. Bangla\n",
            "  12. Belarusian\n",
            "  13. Belgian\n",
            "  14. Berber (Algeria, Latin)\n",
            "  15. Bosnian\n",
            "  16. Braille\n",
            "  17. Bulgarian\n",
            "  18. Burmese\n",
            "  19. Chinese\n",
            "  20. Croatian\n",
            "  21. Czech\n",
            "  22. Danish\n",
            "  23. Dhivehi\n",
            "  24. Dutch\n",
            "  25. Dzongkha\n",
            "  26. English (Australian)\n",
            "  27. English (Cameroon)\n",
            "  28. English (Ghana)\n",
            "  29. English (Nigeria)\n",
            "  30. English (South Africa)\n",
            "  31. English (UK)\n",
            "  32. English (US)\n",
            "  33. Esperanto\n",
            "  34. Estonian\n",
            "  35. Faroese\n",
            "  36. Filipino\n",
            "  37. Finnish\n",
            "  38. French\n",
            "  39. French (Canada)\n",
            "  40. French (Democratic Republic of the Congo)\n",
            "  41. French (Togo)\n",
            "  42. Georgian\n",
            "  43. German\n",
            "  44. German (Austria)\n",
            "  45. Greek\n",
            "  46. Hebrew\n",
            "  47. Hungarian\n",
            "  48. Icelandic\n",
            "  49. Indian\n",
            "  50. Indonesian (Javanese)\n",
            "  51. Indonesian (Latin)\n",
            "  52. Iraqi\n",
            "  53. Irish\n",
            "  54. Italian\n",
            "  55. Japanese\n",
            "  56. Japanese (PC-98)\n",
            "  57. Kazakh\n",
            "  58. Khmer (Cambodia)\n",
            "  59. Korean\n",
            "  60. Kyrgyz\n",
            "  61. Lao\n",
            "  62. Latvian\n",
            "  63. Lithuanian\n",
            "  64. Macedonian\n",
            "  65. Malay (Jawi, Arabic Keyboard)\n",
            "  66. Maltese\n",
            "  67. Maori\n",
            "  68. Moldavian\n",
            "  69. Mongolian\n",
            "  70. Montenegrin\n",
            "  71. Nepali\n",
            "  72. NKo (AZERTY)\n",
            "  73. Norwegian\n",
            "  74. Persian\n",
            "  75. Polish\n",
            "  76. Portuguese\n",
            "  77. Portuguese (Brazil)\n",
            "  78. Romanian\n",
            "  79. Russian\n",
            "  80. Serbian\n",
            "  81. Sinhala (phonetic)\n",
            "  82. Slovak\n",
            "  83. Slovenian\n",
            "  84. Spanish\n",
            "  85. Spanish (Latin American)\n",
            "  86. Swahili (Kenya)\n",
            "  87. Swahili (Tanzania)\n",
            "  88. Swedish\n",
            "  89. Switzerland\n",
            "  90. Taiwanese\n",
            "  91. Tajik\n",
            "  92. Thai\n",
            "  93. Tswana\n",
            "  94. Turkish\n",
            "  95. Turkmen\n",
            "  96. Ukrainian\n",
            "  97. Urdu (Pakistan)\n",
            "  98. Uzbek\n",
            "  99. Vietnamese\n",
            "  100. Wolof\n",
            "\u001b[4mCountry of origin for the keyboard: \u001b[m\u001b[1m32\n",
            "\u001b[m\u001b[m\n",
            "Please select the layout matching the keyboard for this machine.\n",
            "\n",
            "  1. English (US)\n",
            "  2. English (US) - Cherokee\n",
            "  3. English (US) - English (classic Dvorak)\n",
            "  4. English (US) - English (Colemak)\n",
            "  5. English (US) - English (Colemak-DH)\n",
            "  6. English (US) - English (Colemak-DH ISO)\n",
            "  7. English (US) - English (Dvorak)\n",
            "  8. English (US) - English (Dvorak, alt. intl.)\n",
            "  9. English (US) - English (Dvorak, intl., with dead keys)\n",
            "  10. English (US) - English (Dvorak, left-handed)\n",
            "  11. English (US) - English (Dvorak, right-handed)\n",
            "  12. English (US) - English (intl., with AltGr dead keys)\n",
            "  13. English (US) - English (Macintosh)\n",
            "  14. English (US) - English (Norman)\n",
            "  15. English (US) - English (programmer Dvorak)\n",
            "  16. English (US) - English (the divide/multiply toggle the layout)\n",
            "  17. English (US) - English (US, alt. intl.)\n",
            "  18. English (US) - English (US, euro on 5)\n",
            "  19. English (US) - English (US, intl., with dead keys)\n",
            "  20. English (US) - English (US, Symbolic)\n",
            "  21. English (US) - English (Workman)\n",
            "  22. English (US) - English (Workman, intl., with dead keys)\n",
            "  23. English (US) - Hawaiian\n",
            "  24. English (US) - Russian (US, phonetic)\n",
            "  25. English (US) - Serbo-Croatian (US)\n",
            "\u001b[4mKeyboard layout: \u001b[m\u001b[1m1\n",
            "\u001b[m\u001b[m\n",
            "Your console font configuration will be updated the next time your system\n",
            "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
            "Setting up cuda-cupti-11-3 (11.3.58-1) ...\n",
            "Setting up libnvidia-encode-555:amd64 (555.42.06-0ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cupti-dev-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-libraries-dev-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-visual-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-samples-11-3 (11.3.58-1) ...\n",
            "Setting up xserver-xorg-video-nvidia-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-command-line-tools-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-documentation-11-3 (11.3.58-1) ...\n",
            "Setting up dkms (2.8.7-2ubuntu2.2) ...\n",
            "Setting up nvidia-dkms-555 (555.42.06-0ubuntu1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Loading new nvidia-555.42.06 DKMS files...\n",
            "It is likely that 6.1.85+ belongs to a chroot's host\n",
            "Building for 5.15.0-117-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 5.15.0-117-generic\n",
            "Done.\n",
            "\n",
            "nvidia.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "nvidia-peermem.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-117-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "Setting up cuda-tools-11-3 (11.3.0-1) ...\n",
            "Setting up nvidia-driver-555 (555.42.06-0ubuntu1) ...\n",
            "Setting up cuda-toolkit-11-3 (11.3.0-1) ...\n",
            "Setting alternatives\n",
            "update-alternatives: using /usr/local/cuda-11.3 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
            "Setting up cuda-drivers-555 (555.42.06-1) ...\n",
            "Setting up cuda-drivers (555.42.06-1) ...\n",
            "Setting up cuda-runtime-11-3 (11.3.0-1) ...\n",
            "Setting up cuda-demo-suite-11-3 (11.3.58-1) ...\n",
            "Setting up cuda-11-3 (11.3.0-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!export CUDA_PATH=/usr/local/cuda-11.3/\n",
        "#!export PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_PATH=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/bin:$PATH\n",
        "#!export CUDA_HOME=/usr/local/cuda-11.3/"
      ],
      "metadata": {
        "id": "PuthK8ce16jt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claude says that the !export doesn't persist outside of the cell. Whoops."
      ],
      "metadata": {
        "id": "qzBJtnmg9CcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek_OMQGz8aeR",
        "outputId": "75e4edb6-d189-49e2-f5c5-1af8ffa109e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.3'\n",
        "os.environ['PATH'] = '/usr/local/cuda-11.3/bin:' + os.environ['PATH']\n",
        "\n",
        "# THIS WORKED! YAY!!\n",
        "\n",
        "! nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBSdi8Nt9FoS",
        "outputId": "09be833f-3dbc-4190-e64d-c1869d01a42e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Mar_21_19:15:46_PDT_2021\n",
            "Cuda compilation tools, release 11.3, V11.3.58\n",
            "Build cuda_11.3.r11.3/compiler.29745058_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display cuda\n",
        "\n",
        "!sudo update-alternatives --config cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2k5e63-Uwg",
        "outputId": "b1358077-123f-4360-9a24-1570da7a401f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda - auto mode\n",
            "  link best version is /usr/local/cuda-12.2\n",
            "  link currently points to /usr/local/cuda-12.2\n",
            "  link cuda is /usr/local/cuda\n",
            "/usr/local/cuda-11.3 - priority 113\n",
            "/usr/local/cuda-12.2 - priority 122\n",
            "There are 2 choices for the alternative cuda (providing /usr/local/cuda).\n",
            "\n",
            "  Selection    Path                  Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/local/cuda-12.2   122       auto mode\n",
            "  1            /usr/local/cuda-11.3   113       manual mode\n",
            "  2            /usr/local/cuda-12.2   122       manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/local/cuda-11.3 to provide /usr/local/cuda (cuda) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we install a GCC version compatible:"
      ],
      "metadata": {
        "id": "PorxhJIg-1q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check(?)\n",
        "\n",
        "!sudo update-alternatives --remove-all gcc\n",
        "!sudo update-alternatives --remove-all g++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v3F4i02JK1g",
        "outputId": "4a42e8d0-2fb8-4ae7-ab44-fcf690dfebc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: error: no alternatives for gcc\n",
            "update-alternatives: error: no alternatives for g++\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential software-properties-common -y\n",
        "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc-9 g++-9 -y\n",
        "!sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9\n",
        "!gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GaBumXpw-1TR",
        "outputId": "cc2f5578-74c8-485a-a25c-d096c52aef36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r0% [1 InRelease 0 B]\r                    \rIgn:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "\r                    \r0% [Working]\r            \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [2 Release 0 B/564 B 0%]\r0% [Connecting to r2u.stat.illinois.edu]\r                                        \rGet:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "\r0% [2 Release 0 B/564 B 0%] [Connecting to archive.ubuntu.com (185.125.190.82)]\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Ign:13 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:14 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "PPA publishes dbgsym, you may need to include 'main/debug' component\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu/ jammy main'\n",
            "Description:\n",
            "Toolchain test builds; see https://wiki.ubuntu.com/ToolChain\n",
            "\n",
            "More info: https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/ubuntu-toolchain-r-ubuntu-test-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/ubuntu-toolchain-r-ubuntu-test.gpg with fingerprint 60C317803A41BA51845E371A1E9377A2BA9EF27F\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Get:15 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease [24.6 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy/main amd64 Packages [17.4 kB]\n",
            "Fetched 42.0 kB in 2s (22.4 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Get:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-ubuntu2004-11-3-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Get:2 file:/var/cuda-repo-ubuntu2004-11-3-local  Release [564 B]\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Ign:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntu-toolchain-r/test/ubuntu jammy InRelease\n",
            "Hit:14 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-ubuntu2004-11-3-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cpp-9 gcc-9-base libasan5 libgcc-9-dev libstdc++-9-dev\n",
            "Suggested packages:\n",
            "  gcc-9-locales g++-9-multilib gcc-9-doc gcc-9-multilib libstdc++-9-doc\n",
            "The following NEW packages will be installed:\n",
            "  cpp-9 g++-9 gcc-9 gcc-9-base libasan5 libgcc-9-dev libstdc++-9-dev\n",
            "0 upgraded, 7 newly installed, 0 to remove and 61 not upgraded.\n",
            "Need to get 41.2 MB of archives.\n",
            "After this operation, 138 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gcc-9-base amd64 9.5.0-1ubuntu1~22.04 [19.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 cpp-9 amd64 9.5.0-1ubuntu1~22.04 [10.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libasan5 amd64 9.5.0-1ubuntu1~22.04 [3,140 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgcc-9-dev amd64 9.5.0-1ubuntu1~22.04 [2,520 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gcc-9 amd64 9.5.0-1ubuntu1~22.04 [11.3 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libstdc++-9-dev amd64 9.5.0-1ubuntu1~22.04 [1,824 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 g++-9 amd64 9.5.0-1ubuntu1~22.04 [11.9 MB]\n",
            "Fetched 41.2 MB in 4s (10.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-9-base:amd64.\n",
            "(Reading database ... 132513 files and directories currently installed.)\n",
            "Preparing to unpack .../0-gcc-9-base_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-9-base:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package cpp-9.\n",
            "Preparing to unpack .../1-cpp-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan5:amd64.\n",
            "Preparing to unpack .../2-libasan5_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan5:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-9-dev:amd64.\n",
            "Preparing to unpack .../3-libgcc-9-dev_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-9.\n",
            "Preparing to unpack .../4-gcc-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libstdc++-9-dev:amd64.\n",
            "Preparing to unpack .../5-libstdc++-9-dev_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libstdc++-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package g++-9.\n",
            "Preparing to unpack .../6-g++-9_9.5.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking g++-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up gcc-9-base:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libasan5:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up cpp-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libgcc-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up gcc-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up libstdc++-9-dev:amd64 (9.5.0-1ubuntu1~22.04) ...\n",
            "Setting up g++-9 (9.5.0-1ubuntu1~22.04) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "update-alternatives: using /usr/bin/gcc-9 to provide /usr/bin/gcc (gcc) in auto mode\n",
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fixed it again! yay!\n",
        "\n",
        "os.environ['CC'] = '/usr/bin/gcc-9'\n",
        "os.environ['CXX'] = '/usr/bin/g++-9'\n",
        "os.environ['CUDAHOSTCXX'] = '/usr/bin/g++-9'"
      ],
      "metadata": {
        "id": "yYnMcbidHsgR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works, but you will have to deal with this girthy installation every time you want to use this notebook."
      ],
      "metadata": {
        "id": "VQUnsKUL9SMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trying out installing torch-int inside smoothquant to see if that solves the module recognition problem\n",
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1188LC3LtZYt",
        "outputId": "4abe7cde-1667-4baa-d1f1-1f39b08e6e65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Guangxuan-Xiao/torch-int.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4AytQdgi_gj",
        "outputId": "39d9b3b6-d8b0-4769-d4a7-c984d082c8f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-int'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 1102 (delta 139), reused 118 (delta 118), pack-reused 929\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 960.91 KiB | 20.89 MiB/s, done.\n",
            "Resolving deltas: 100% (643/643), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torch-int"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjJqyK7jwSP",
        "outputId": "5b6b3c6a-801a-4e17-f603-dac0833c5c92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also had to jump through some hoops trying to figure out this submodule stuffâ€”it is not exactly what the repo says to do"
      ],
      "metadata": {
        "id": "FbQYofhC94cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is probably where the fix is going to be\n",
        "\n",
        "! git config submodule.submodules/cutlass.url https://github.com/NVIDIA/cutlass.git\n",
        "! git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI103aFzp_RG",
        "outputId": "e7cb57b2-198c-4aff-d00f-2dbbd8280a71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/torch-int/submodules/cutlass'...\n",
            "Submodule path 'submodules/cutlass': checked out 'c975e2ccbb2dbf13024568b37ffa3498ed0b3aed'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gcc -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2R7WxtNCBYZ",
        "outputId": "ce10dea3-9b6f-4a01-a5a7-7ca101509453"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using built-in specs.\n",
            "COLLECT_GCC=gcc\n",
            "COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/9/lto-wrapper\n",
            "OFFLOAD_TARGET_NAMES=nvptx-none:hsa\n",
            "OFFLOAD_TARGET_DEFAULT=1\n",
            "Target: x86_64-linux-gnu\n",
            "Configured with: ../src/configure -v --with-pkgversion='Ubuntu 9.5.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-9/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,gm2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-9 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-9-5Q4PKF/gcc-9-9.5.0/debian/tmp-nvptx/usr,hsa --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-mutex\n",
            "Thread model: posix\n",
            "gcc version 9.5.0 (Ubuntu 9.5.0-1ubuntu1~22.04) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yet, when I try to build_cutlass.sh, the CUDA compiler is still identified as 12.2.140. The way to fix this is the cell above, changing the alternatives for CUDA usage"
      ],
      "metadata": {
        "id": "oREC2Ch-9xRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --display gcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1zmiVBCElD",
        "outputId": "40301eea-0a3a-473a-fcaa-f2e01a340395"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc - auto mode\n",
            "  link best version is /usr/bin/gcc-9\n",
            "  link currently points to /usr/bin/gcc-9\n",
            "  link gcc is /usr/bin/gcc\n",
            "  slave g++ is /usr/bin/g++\n",
            "/usr/bin/gcc-9 - priority 90\n",
            "  slave g++: /usr/bin/g++-9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This still gave me errors for a bit, because you need to use a GCC compatible with the different CUDA version. But even when I installed a new GCC version, it seemed to use it but didn't? Weird."
      ],
      "metadata": {
        "id": "LM_FmyANCmtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! source environment.sh\n",
        "! CC=/usr/bin/gcc-9 CXX=/usr/bin/g++-9 bash build_cutlass.sh   # this one needs GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DSE8hTjCIU",
        "outputId": "fe5b90e1-d940-4d6b-f368-f8573a818b5e",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- CMake Version: 3.30.1\n",
            "-- The CXX compiler identification is GNU 9.5.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- The CUDA compiler identification is NVIDIA 11.3.58\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- CUDART: /usr/local/cuda/lib64/libcudart.so\n",
            "-- CUDA Driver: /usr/local/cuda/lib64/stubs/libcuda.so\n",
            "-- NVRTC: /usr/local/cuda/lib64/libnvrtc.so\n",
            "-- Default Install Location: install\n",
            "-- CUDA Compilation Architectures: 80\n",
            "-- Enable caching of reference results in conv unit tests\n",
            "-- Enable rigorous conv problem sizes in conv unit tests\n",
            "-- Using NVCC flags: -DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;$<$<BOOL:1>:-Xcompiler=-Wconversion>;$<$<BOOL:1>:-Xcompiler=-fno-strict-aliasing>\n",
            "-- CUTLASS Revision: c975e2cc\n",
            "-- Configuring cublas ...\n",
            "-- cuBLAS Disabled.\n",
            "-- Configuring cuBLAS ... done.\n",
            "-- Found Python3: /usr/local/bin/python3.10 (found suitable version \"3.10.13\", minimum required is \"3.5\") found components: Interpreter\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c09b164b76dc.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.1134c512275c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.70bca42e032c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7412477fb00b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7cb784ae67c3.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.452e17cedfde.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.21cd46163fea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.f2656f9851f5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.da510d6f1a23.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed2bb5981d55.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.22f9e56a890e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.17417f539ca2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.d1a5010bad63.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5a217a522f90.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e764a5c32d2e.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.00a490a48b54.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.67748a1f69fd.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.aaae24350764.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a7556fb0c363.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.cdd7679105ea.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5b5abd9da9bb.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0188c1e5012f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c1136defa56d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.4efc263febd5.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b1c066c89ad1.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.059188bf387f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dc6fdf4e3631.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ed319cdaa486.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.7f69d119038b.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ab4c8725d89a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.db0920d13551.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.53863dce2b34.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.5206989612aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a40fdb431023.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.999493302618.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.bbd766ae1258.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.3aca44f87dc7.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.fe5af8d45b59.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2d9ae5d87c7a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8112247fe822.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.447ac2531506.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b7cdd3426b6c.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.b83304556123.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.c2bae868a773.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.51d0f833ea6d.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.dd6b23b6f933.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.eac06d5a86ae.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ec9d28d7c560.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.25dee0d0208a.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.41f602329ae6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.9763c0db7135.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.2fdf36a298c6.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.0a8164c28c5f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a4cfc29c733f.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.8a2ca8aaf7aa.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.977a56f2bc38.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.e837fe821ee2.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.6f06994db371.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.ae96169b4e45.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.a6a0ceb84086.cu\n",
            "-- Generating /content/torch-int/submodules/cutlass/build/tools/library/cutlass_library_objs.unity.19976851ec6d.cu\n",
            "-- Configuring done (3.5s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/torch-int/submodules/cutlass/build\n",
            "[  0%] \u001b[32mBuilding CUDA object examples/00_basic_gemm/CMakeFiles/00_basic_gemm.dir/basic_gemm.cu.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CUDA object examples/01_cutlass_utilities/CMakeFiles/01_cutlass_utilities.dir/cutlass_utilities.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/02_dump_reg_shmem/CMakeFiles/02_dump_reg_shmem.dir/dump_reg_shmem.cu.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CUDA object examples/04_tile_iterator/CMakeFiles/04_tile_iterator.dir/tile_iterator.cu.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CUDA object examples/05_batched_gemm/CMakeFiles/05_batched_gemm.dir/batched_gemm.cu.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CUDA object examples/06_splitK_gemm/CMakeFiles/06_splitK_gemm.dir/splitk_gemm.cu.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/visualize_layout.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object examples/03_visualize_layout/CMakeFiles/03_visualize_layout.dir/register_layout.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object examples/08_turing_tensorop_gemm/CMakeFiles/08_turing_tensorop_gemm.dir/turing_tensorop_gemm.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/handle.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object examples/09_turing_tensorop_conv2dfprop/CMakeFiles/09_turing_tensorop_conv2dfprop.dir/turing_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/07_volta_tensorop_gemm/CMakeFiles/07_volta_tensorop_gemm.dir/volta_tensorop_gemm.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/12_gemm_bias_relu/CMakeFiles/12_gemm_bias_relu.dir/gemm_bias_relu.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_rf.dir/fused_two_convs_f16_sm80_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_shmem.dir/fused_two_convs_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm75_rf.dir/fused_two_convs_f16_sm75_rf.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/src/manifest.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/operation_table.cu.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CUDA executable 04_tile_iterator\u001b[0m\n",
            "[  9%] Built target 04_tile_iterator\n",
            "[ 10%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_f16_sm80_shmem.dir/fused_two_convs_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_rf.dir/fused_two_convs_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CUDA executable 02_dump_reg_shmem\u001b[0m\n",
            "[ 11%] Built target 02_dump_reg_shmem\n",
            "[ 12%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm75_shmem.dir/fused_two_convs_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/singleton.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32m\u001b[1mLinking CUDA executable 00_basic_gemm\u001b[0m\n",
            "[ 12%] Built target 00_basic_gemm\n",
            "[ 13%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_rf.dir/fused_two_convs_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CUDA executable 08_turing_tensorop_gemm\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CXX executable 03_visualize_layout\u001b[0m\n",
            "[ 14%] Built target 08_turing_tensorop_gemm\n",
            "[ 15%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_convs_s8_sm80_shmem.dir/fused_two_convs_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 15%] Built target 03_visualize_layout\n",
            "[ 15%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_rf.dir/fused_two_gemms_f16_sm75_rf.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/util.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 12_gemm_bias_relu\u001b[0m\n",
            "[ 17%] Built target 12_gemm_bias_relu\n",
            "[ 17%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm75_shmem.dir/fused_two_gemms_f16_sm75_shmem.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 01_cutlass_utilities\u001b[0m\n",
            "[ 17%] Built target 01_cutlass_utilities\n",
            "[ 17%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_rf.dir/fused_two_gemms_f16_sm80_rf.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 09_turing_tensorop_conv2dfprop\u001b[0m\n",
            "[ 17%] Built target 09_turing_tensorop_conv2dfprop\n",
            "[ 17%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_f16_sm80_shmem.dir/fused_two_gemms_f16_sm80_shmem.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CUDA executable 05_batched_gemm\u001b[0m\n",
            "[ 17%] Built target 05_batched_gemm\n",
            "[ 17%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_rf.dir/fused_two_gemms_s8_sm75_rf.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/gemm.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_rf\u001b[0m\n",
            "[ 18%] Built target 13_fused_two_convs_f16_sm75_rf\n",
            "[ 19%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/initialize_reference_operations.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_rf\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm75_shmem\u001b[0m\n",
            "[ 20%] Built target 13_fused_two_convs_f16_sm80_rf\n",
            "[ 20%] Built target 13_fused_two_convs_f16_sm75_shmem\n",
            "[ 21%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm75_shmem.dir/fused_two_gemms_s8_sm75_shmem.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_rf.dir/fused_two_gemms_s8_sm80_rf.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/reduction_device.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_f16_sm80_shmem\u001b[0m\n",
            "[ 22%] Built target 13_fused_two_convs_f16_sm80_shmem\n",
            "[ 23%] \u001b[32mBuilding CUDA object examples/13_two_tensor_op_fusion/CMakeFiles/13_fused_two_gemms_s8_sm80_shmem.dir/fused_two_gemms_s8_sm80_shmem.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_rf\u001b[0m\n",
            "[ 24%] Built target 13_fused_two_gemms_f16_sm75_rf\n",
            "[ 25%] \u001b[32mBuilding CUDA object examples/14_ampere_tf32_tensorop_gemm/CMakeFiles/14_ampere_tf32_tensorop_gemm.dir/ampere_tf32_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_rf\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm75_shmem\u001b[0m\n",
            "[ 25%] Built target 13_fused_two_convs_s8_sm75_rf\n",
            "[ 26%] \u001b[32mBuilding CUDA object examples/15_ampere_sparse_tensorop_gemm/CMakeFiles/15_ampere_sparse_tensorop_gemm.dir/ampere_sparse_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 26%] Built target 13_fused_two_convs_s8_sm75_shmem\n",
            "[ 26%] \u001b[32mBuilding CUDA object examples/16_ampere_tensorop_conv2dfprop/CMakeFiles/16_ampere_tensorop_conv2dfprop.dir/ampere_tensorop_conv2dfprop.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reduction/init_reduction_operations.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm75_shmem\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_rf\u001b[0m\n",
            "[ 29%] Built target 13_fused_two_gemms_f16_sm75_shmem\n",
            "[ 29%] \u001b[32mBuilding CUDA object examples/17_fprop_per_channel_bias/CMakeFiles/17_fprop_per_channel_bias.dir/fprop_per_channel_bias.cu.o\u001b[0m\n",
            "[ 29%] Built target 13_fused_two_gemms_f16_sm80_rf\n",
            "[ 29%] \u001b[32mBuilding CUDA object examples/18_ampere_fp64_tensorop_affine2_gemm/CMakeFiles/18_ampere_fp64_tensorop_affine2_gemm.dir/ampere_fp64_tensorop_affine2_gemm.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_rf\u001b[0m\n",
            "[ 29%] Built target 13_fused_two_gemms_s8_sm75_rf\n",
            "[ 29%] \u001b[32mBuilding CUDA object examples/19_tensorop_canonical/CMakeFiles/19_tensorop_canonical.dir/tensorop_canonical.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_f16_sm80_shmem\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_gemms_f16_sm80_shmem\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/20_simt_canonical/CMakeFiles/20_simt_canonical.dir/simt_canonical.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_rf\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_convs_s8_sm80_rf\n",
            "[ 30%] \u001b[32mBuilding CUDA object examples/21_quaternion_gemm/CMakeFiles/21_quaternion_gemm.dir/quaternion_gemm.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv2d.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_convs_s8_sm80_shmem\u001b[0m\n",
            "[ 30%] Built target 13_fused_two_convs_s8_sm80_shmem\n",
            "[ 31%] \u001b[32mBuilding CUDA object examples/22_quaternion_conv/CMakeFiles/22_quaternion_conv.dir/quaternion_conv.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CUDA executable 19_tensorop_canonical\u001b[0m\n",
            "[ 32%] Built target 19_tensorop_canonical\n",
            "[ 33%] \u001b[32mBuilding CUDA object examples/23_ampere_gemm_operand_reduction_fusion/CMakeFiles/23_ampere_gemm_operand_reduction_fusion.dir/ampere_gemm_operand_reduction_fusion.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CUDA executable 20_simt_canonical\u001b[0m\n",
            "[ 34%] Built target 20_simt_canonical\n",
            "[ 35%] \u001b[32mBuilding CUDA object examples/24_gemm_grouped/CMakeFiles/24_gemm_grouped.dir/gemm_grouped.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_rf\u001b[0m\n",
            "[ 35%] Built target 13_fused_two_gemms_s8_sm80_rf\n",
            "[ 36%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_fprop_mainloop_fusion.dir/ampere_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA executable 14_ampere_tf32_tensorop_gemm\u001b[0m\n",
            "[ 36%] Built target 14_ampere_tf32_tensorop_gemm\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA executable 15_ampere_sparse_tensorop_gemm\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CUDA object examples/25_ampere_fprop_mainloop_fusion/CMakeFiles/25_ampere_3d_fprop_mainloop_fusion.dir/ampere_3d_fprop_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 37%] Built target 15_ampere_sparse_tensorop_gemm\n",
            "[ 37%] \u001b[32mBuilding CUDA object examples/26_ampere_wgrad_mainloop_fusion/CMakeFiles/26_ampere_wgrad_mainloop_fusion.dir/ampere_wgrad_mainloop_fusion.cu.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm75_shmem\u001b[0m\n",
            "[ 37%] Built target 13_fused_two_gemms_s8_sm75_shmem\n",
            "[ 37%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/src/reference/conv3d.cu.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CUDA executable 18_ampere_fp64_tensorop_affine2_gemm\u001b[0m\n",
            "[ 38%] Built target 18_ampere_fp64_tensorop_affine2_gemm\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CUDA executable 16_ampere_tensorop_conv2dfprop\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CUDA object examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/CMakeFiles/27_ampere_3xtf32_fast_accurate_tensorop_gemm.dir/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu.o\u001b[0m\n",
            "[ 39%] Built target 16_ampere_tensorop_conv2dfprop\n",
            "[ 39%] \u001b[32mBuilding CUDA object examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/CMakeFiles/28_ampere_3xtf32_fast_accurate_tensorop_fprop.dir/ampere_3xtf32_fast_accurate_tensorop_fprop.cu.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CUDA executable 17_fprop_per_channel_bias\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CUDA executable 21_quaternion_gemm\u001b[0m\n",
            "[ 40%] Built target 17_fprop_per_channel_bias\n",
            "[ 41%] \u001b[32mBuilding CXX object tools/library/CMakeFiles/cutlass_library_objs.dir/generated/initialize_all.cpp.o\u001b[0m\n",
            "[ 41%] Built target 21_quaternion_gemm\n",
            "[ 41%] \u001b[32mBuilding CUDA object examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/CMakeFiles/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.dir/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c09b164b76dc.cu.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CUDA executable 13_fused_two_gemms_s8_sm80_shmem\u001b[0m\n",
            "[ 41%] Built target 13_fused_two_gemms_s8_sm80_shmem\n",
            "[ 41%] \u001b[32mBuilding CUDA object examples/30_wgrad_split_k/CMakeFiles/30_wgrad_split_k.dir/30_wgrad_split_k.cu.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CUDA executable 23_ampere_gemm_operand_reduction_fusion\u001b[0m\n",
            "[ 41%] Built target 23_ampere_gemm_operand_reduction_fusion\n",
            "[ 42%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.1134c512275c.cu.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CUDA executable 22_quaternion_conv\u001b[0m\n",
            "[ 42%] Built target 22_quaternion_conv\n",
            "[ 43%] \u001b[32mBuilding CUDA object examples/31_basic_syrk/CMakeFiles/31_basic_syrk.dir/basic_syrk.cu.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_fprop_mainloop_fusion\u001b[0m\n",
            "[ 43%] Built target 25_ampere_fprop_mainloop_fusion\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable 26_ampere_wgrad_mainloop_fusion\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CUDA object examples/32_basic_trmm/CMakeFiles/32_basic_trmm.dir/basic_trmm.cu.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CUDA executable 25_ampere_3d_fprop_mainloop_fusion\u001b[0m\n",
            "[ 45%] Built target 26_ampere_wgrad_mainloop_fusion\n",
            "[ 46%] \u001b[32mBuilding CUDA object examples/33_ampere_3xtf32_tensorop_symm/CMakeFiles/33_ampere_3xtf32_tensorop_symm.dir/ampere_3xtf32_tensorop_symm.cu.o\u001b[0m\n",
            "[ 46%] Built target 25_ampere_3d_fprop_mainloop_fusion\n",
            "[ 46%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.70bca42e032c.cu.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CUDA executable 27_ampere_3xtf32_fast_accurate_tensorop_gemm\u001b[0m\n",
            "[ 47%] Built target 27_ampere_3xtf32_fast_accurate_tensorop_gemm\n",
            "[ 48%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7412477fb00b.cu.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CUDA executable 30_wgrad_split_k\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CUDA executable 28_ampere_3xtf32_fast_accurate_tensorop_fprop\u001b[0m\n",
            "[ 49%] Built target 30_wgrad_split_k\n",
            "[ 50%] \u001b[32mBuilding CUDA object examples/34_transposed_conv2d/CMakeFiles/34_transposed_conv2d.dir/34_transposed_conv2d.cu.o\u001b[0m\n",
            "[ 50%] Built target 28_ampere_3xtf32_fast_accurate_tensorop_fprop\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/35_gemm_softmax/CMakeFiles/35_gemm_softmax.dir/gemm_softmax.cu.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CUDA executable 31_basic_syrk\u001b[0m\n",
            "[ 51%] Built target 31_basic_syrk\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/36_gather_scatter_fusion/CMakeFiles/36_gather_scatter_fusion.dir/gather_scatter_fusion.cu.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CUDA executable 32_basic_trmm\u001b[0m\n",
            "[ 51%] Built target 32_basic_trmm\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/37_gemm_layernorm_gemm_fusion/CMakeFiles/37_gemm_layernorm_gemm_fusion.dir/gemm_layernorm.cu.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CUDA executable 24_gemm_grouped\u001b[0m\n",
            "[ 51%] Built target 24_gemm_grouped\n",
            "[ 51%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7cb784ae67c3.cu.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CUDA object examples/38_syr2k_grouped/CMakeFiles/38_syr2k_grouped.dir/syr2k_grouped.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(507): warning: variable \"minus\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(508): warning: variable \"mul\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h(509): warning: variable \"exponential\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::visit(int, int, int, int, const cutlass::kernel::EpilogueVisitorLayerNorm<ThreadblockShape_, ThreadCount, OutputTileIterator_, AccumulatorTile_, ElementAccumulator_, ElementVariance_, ElementMean_, ElementLayernormCompute_, ElementwiseFunctor_, IsShiftedVariance_>::AccumulatorFragment &) [with ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, ThreadCount=128, OutputTileIterator_=cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, AccumulatorTile_=cutlass::Array<cutlass::half_t, 128, false>, ElementAccumulator_=cutlass::half_t, ElementVariance_=cutlass::half_t, ElementMean_=cutlass::half_t, ElementLayernormCompute_=float, ElementwiseFunctor_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h(328): here\n",
            "            instantiation of \"void cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::operator()(cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::Visitor &, const cutlass::epilogue::threadblock::EpilogueWithVisitor<Visitor_, Shape_, WarpMmaOperator_, PartitionsK, AccumulatorFragmentIterator_, WarpTileIterator_, SharedLoadIterator_, Padding_, FragmentsPerPartition, IterationsUnroll>::AccumulatorTile &) [with Visitor_=cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, Shape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpMmaOperator_=cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, PartitionsK=1, AccumulatorFragmentIterator_=cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, WarpTileIterator_=cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, SharedLoadIterator_=cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, Padding_=cutlass::MatrixShape<0, 16>, FragmentsPerPartition=1, IterationsUnroll=1]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h(434): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::operator()(const cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::Params &, cutlass::gemm::kernel::GemmWithEpilogueVisitor<Mma_, Epilogue_, ThreadblockSwizzle_>::SharedStorage &) [with Mma_=cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, Epilogue_=cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, ThreadblockSwizzle_=cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::GemmWithEpilogueVisitor<cutlass::gemm::threadblock::MmaMultistage<cutlass::gemm::GemmShape<128, 128, 32>, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajor, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<128, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::transform::threadblock::PredicatedTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, 1, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<32, 128>, 128, cutlass::layout::PitchLinearShape<4, 8>, 8>, 16>, cutlass::arch::CacheOperation::Global, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, 3, cutlass::gemm::SharedMemoryClearOption::kNone, __nv_bool>, cutlass::epilogue::threadblock::EpilogueWithVisitor<cutlass::kernel::EpilogueVisitorLayerNorm<cutlass::gemm::GemmShape<128, 128, 32>, 128, cutlass::epilogue::threadblock::PredicatedTileIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>, cutlass::half_t, false, cutlass::layout::NoPermute, false>, cutlass::Array<cutlass::half_t, 128, false>, cutlass::half_t, cutlass::half_t, cutlass::half_t, float, cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, false>, cutlass::gemm::GemmShape<128, 128, 32>, cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 16>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, 1, cutlass::epilogue::warp::FragmentIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::Array<cutlass::half_t, 4, false>, cutlass::layout::RowMajor>, cutlass::epilogue::warp::TileIteratorTensorOp<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::half_t, cutlass::layout::RowMajor>, cutlass::epilogue::threadblock::SharedLoadIterator<cutlass::epilogue::threadblock::OutputTileOptimalThreadMap<cutlass::epilogue::threadblock::OutputTileShape<128, 8, 2, 1, 1>, cutlass::epilogue::threadblock::OutputTileShape<1, 8, 1, 1, 8>, 128, 8, 16>::CompactedThreadMap, cutlass::half_t, 16>, cutlass::MatrixShape<0, 16>, 1, 1>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>>]\" \n",
            "(1010): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::run(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "(1058): here\n",
            "            instantiation of \"cutlass::Status cutlass::GemmLayernorm<ElementInputA0_, LayoutInputA0_, ElementInputB0_, LayoutInputB0_, ElementOutput_, LayoutOutput_, ElementCompute_, EpilogueFunctorOp_, ThreadblockShape_, WarpShape_, InstructionShape_, Stages0, Stages1, IsShiftedVariance_>::operator()(cudaStream_t) [with ElementInputA0_=cutlass::half_t, LayoutInputA0_=cutlass::layout::RowMajor, ElementInputB0_=cutlass::half_t, LayoutInputB0_=cutlass::layout::ColumnMajor, ElementOutput_=cutlass::half_t, LayoutOutput_=cutlass::layout::ColumnMajor, ElementCompute_=cutlass::half_t, EpilogueFunctorOp_=cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 8, cutlass::half_t, cutlass::half_t, cutlass::epilogue::thread::ScaleType::Default, cutlass::FloatRoundStyle::round_to_nearest>, ThreadblockShape_=cutlass::gemm::GemmShape<128, 128, 32>, WarpShape_=cutlass::gemm::GemmShape<64, 64, 32>, InstructionShape_=cutlass::gemm::GemmShape<16, 8, 16>, Stages0=3, Stages1=4, IsShiftedVariance_=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(559): here\n",
            "            instantiation of \"cutlass::Status Testbed<LayoutOutput_>::execute_device_kernel() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(406): here\n",
            "            instantiation of \"Disposition Testbed<LayoutOutput_>::run() [with LayoutOutput_=cutlass::layout::ColumnMajor]\" \n",
            "/content/torch-int/submodules/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu(917): here\n",
            "\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CUDA executable 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\u001b[0m\n",
            "[ 52%] Built target 29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm\n",
            "[ 52%] \u001b[32mBuilding CUDA object examples/39_gemm_permute/CMakeFiles/39_gemm_permute.dir/gemm_permute.cu.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CUDA executable 34_transposed_conv2d\u001b[0m\n",
            "[ 52%] Built target 34_transposed_conv2d\n",
            "[ 53%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.452e17cedfde.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 06_splitK_gemm\u001b[0m\n",
            "[ 53%] Built target 06_splitK_gemm\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_fixed_seqlen.dir/fused_multihead_attention_fixed_seqlen.cu.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CUDA executable 07_volta_tensorop_gemm\u001b[0m\n",
            "[ 53%] Built target 07_volta_tensorop_gemm\n",
            "[ 53%] \u001b[32mBuilding CUDA object examples/41_fused_multi_head_attention/CMakeFiles/41_fused_multi_head_attention_variable_seqlen.dir/fused_multihead_attention_variable_seqlen.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 36_gather_scatter_fusion\u001b[0m\n",
            "[ 54%] Built target 36_gather_scatter_fusion\n",
            "[ 54%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.21cd46163fea.cu.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 33_ampere_3xtf32_tensorop_symm\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CUDA executable 35_gemm_softmax\u001b[0m\n",
            "[ 54%] Built target 33_ampere_3xtf32_tensorop_symm\n",
            "[ 55%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.f2656f9851f5.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1081): here\n",
            "\n",
            "[ 55%] Built target 35_gemm_softmax\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1083): here\n",
            "\n",
            "[ 56%] \u001b[32mBuilding CUDA object examples/42_ampere_tensorop_group_conv/CMakeFiles/42_ampere_tensorop_group_conv.dir/ampere_tensorop_group_conv.cu.o\u001b[0m\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(704): here\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h(483): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::attention_kernel(AttentionKernel<scalar_t_, ArchTag, isAligned_, kQueriesPerBlock, kKeysPerBlock, kSingleValueIteration>::Params &) [with scalar_t_=cutlass::half_t, ArchTag=cutlass::arch::Sm80, isAligned_=true, kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "(858): here\n",
            "            instantiation of \"void attention_kernel_batched_impl<AK>(AK::Params) [with AK=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(860): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile_grouped() [with Attention=AttentionKernel<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1005): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu(1088): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<32, 128, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<32, 128, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 128>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<128, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=3, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1183): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 32, 128, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, false, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=32, kKeysPerBlock=128, kSingleValueIteration=false]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1185): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h(700): warning: missing return statement at end of non-void function \"cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\"\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<Shape1_, WarpIteratorA1_, AccumulatorSharedStorage, IteratorB1_, SmemIteratorB1_, CacheOpB1, ElementC_, LayoutC_, Policy1_, Stages_, Enable>::set_prologue_done(__nv_bool) [with Shape1_=cutlass::gemm::GemmShape<64, 64, 32>, WarpIteratorA1_=cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<cutlass::MatrixShape<32, 32>, cutlass::gemm::Operand::kA, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<16, 8>, 1, 32, false, 1>, AccumulatorSharedStorage=cutlass::gemm::threadblock::AccumulatorSharedStorage<cutlass::gemm::GemmShape<64, 64, 32>, cutlass::half_t, cutlass::layout::RowMajor, cutlass::MatrixShape<0, 8>>, IteratorB1_=cutlass::transform::threadblock::PredicatedTileAccessIteratorResidualLast<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajor, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, cutlass::Array<cutlass::half_t, 8, false>, false>, SmemIteratorB1_=cutlass::transform::threadblock::RegularTileAccessIterator<cutlass::MatrixShape<32, 64>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, 0, cutlass::transform::PitchLinearWarpRakedThreadMap<cutlass::layout::PitchLinearShape<64, 32>, 128, cutlass::layout::PitchLinearShape<8, 4>, 8>, 16>, CacheOpB1=cutlass::arch::CacheOperation::Global, ElementC_=float, LayoutC_=cutlass::layout::RowMajor, Policy1_=cutlass::gemm::threadblock::MmaPolicy<cutlass::gemm::warp::MmaTensorOp<cutlass::gemm::GemmShape<32, 32, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>, cutlass::half_t, cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>, float, cutlass::layout::RowMajor, cutlass::gemm::warp::MmaTensorOpPolicy<cutlass::arch::Mma<cutlass::gemm::GemmShape<16, 8, 8>, 32, cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>, cutlass::MatrixShape<1, 1>>, 1, false, __nv_bool>, cutlass::MatrixShape<0, 0>, cutlass::MatrixShape<0, 0>, 1>, Stages_=2, Enable=__nv_bool]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(706): here\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kDeviceOnly]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1108): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h(467): warning: variable \"si\" was declared but never referenced\n",
            "          detected during:\n",
            "            instantiation of \"void cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::operator()(const cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::Params &, cutlass::gemm::kernel::FMHAGrouped<MM0_, MM1_, scalar_t_, accum_t_, output_t_, output_accum_t_, kKeepOutputInRF, GroupScheduleMode_>::SharedStorage &) [with MM0_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, MM1_=cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, scalar_t_=cutlass::half_t, accum_t_=float, output_t_=cutlass::half_t, output_accum_t_=float, kKeepOutputInRF=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/device_kernel.h(57): here\n",
            "            instantiation of \"void cutlass::Kernel<Operator>(Operator::Params) [with Operator=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(224): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::maximum_active_blocks(int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/include/cutlass/gemm/device/base_grouped.h(318): here\n",
            "            instantiation of \"int cutlass::gemm::device::BaseGrouped<BaseKernel_>::sufficient(const cutlass::gemm::GemmCoord *, int, int) [with BaseKernel_=cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(895): here\n",
            "            instantiation of \"Result TestbedAttention<Attention>::profile() [with Attention=cutlass::gemm::device::GemmGrouped<cutlass::gemm::kernel::FMHAGrouped<cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM0, cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, 64, 64, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>::MM1, cutlass::half_t, float, cutlass::half_t, float, true, cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute>>]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1086): here\n",
            "            instantiation of \"int run_grouped<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration,GroupScheduleMode_>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true, GroupScheduleMode_=cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1113): here\n",
            "            instantiation of \"int run_attention<kQueriesPerBlock,kKeysPerBlock,kSingleValueIteration>(Options &) [with kQueriesPerBlock=64, kKeysPerBlock=64, kSingleValueIteration=true]\" \n",
            "/content/torch-int/submodules/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu(1190): here\n",
            "\n",
            "[ 56%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.da510d6f1a23.cu.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CUDA executable 37_gemm_layernorm_gemm_fusion\u001b[0m\n",
            "[ 57%] Built target 37_gemm_layernorm_gemm_fusion\n",
            "[ 58%] \u001b[32mBuilding CUDA object examples/43_ell_block_sparse_gemm/CMakeFiles/43_ell_block_sparse_gemm.dir/ell_block_sparse_gemm.cu.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CUDA executable 39_gemm_permute\u001b[0m\n",
            "[ 59%] Built target 39_gemm_permute\n",
            "[ 60%] \u001b[32mBuilding CUDA object examples/45_dual_gemm/CMakeFiles/45_dual_gemm.dir/dual_gemm.cu.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CUDA executable 38_syr2k_grouped\u001b[0m\n",
            "[ 61%] Built target 38_syr2k_grouped\n",
            "[ 62%] \u001b[32mBuilding CUDA object examples/46_depthwise_simt_conv2dfprop/CMakeFiles/46_depthwise_simt_conv2dfprop.dir/depthwise_simt_conv2dfprop.cu.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CUDA executable 42_ampere_tensorop_group_conv\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CUDA executable 43_ell_block_sparse_gemm\u001b[0m\n",
            "[ 62%] Built target 42_ampere_tensorop_group_conv\n",
            "[ 62%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed2bb5981d55.cu.o\u001b[0m\n",
            "[ 62%] Built target 43_ell_block_sparse_gemm\n",
            "[ 63%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.22f9e56a890e.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CUDA executable 46_depthwise_simt_conv2dfprop\u001b[0m\n",
            "[ 63%] Built target 46_depthwise_simt_conv2dfprop\n",
            "[ 63%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.17417f539ca2.cu.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CUDA executable 45_dual_gemm\u001b[0m\n",
            "[ 63%] Built target 45_dual_gemm\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.d1a5010bad63.cu.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5a217a522f90.cu.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_fixed_seqlen\u001b[0m\n",
            "[ 65%] Built target 41_fused_multi_head_attention_fixed_seqlen\n",
            "[ 66%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e764a5c32d2e.cu.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.00a490a48b54.cu.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.67748a1f69fd.cu.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.aaae24350764.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a7556fb0c363.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.cdd7679105ea.cu.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5b5abd9da9bb.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0188c1e5012f.cu.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CUDA executable 41_fused_multi_head_attention_variable_seqlen\u001b[0m\n",
            "[ 69%] Built target 41_fused_multi_head_attention_variable_seqlen\n",
            "[ 69%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c1136defa56d.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.4efc263febd5.cu.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b1c066c89ad1.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.059188bf387f.cu.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dc6fdf4e3631.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ed319cdaa486.cu.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.7f69d119038b.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ab4c8725d89a.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.db0920d13551.cu.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.53863dce2b34.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.5206989612aa.cu.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a40fdb431023.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.999493302618.cu.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.bbd766ae1258.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.3aca44f87dc7.cu.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.fe5af8d45b59.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2d9ae5d87c7a.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8112247fe822.cu.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.447ac2531506.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b7cdd3426b6c.cu.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.b83304556123.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.c2bae868a773.cu.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.51d0f833ea6d.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.dd6b23b6f933.cu.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.eac06d5a86ae.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ec9d28d7c560.cu.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.25dee0d0208a.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.41f602329ae6.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.9763c0db7135.cu.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.2fdf36a298c6.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.0a8164c28c5f.cu.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a4cfc29c733f.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.8a2ca8aaf7aa.cu.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.977a56f2bc38.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.e837fe821ee2.cu.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.6f06994db371.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.ae96169b4e45.cu.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.a6a0ceb84086.cu.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CUDA object tools/library/CMakeFiles/cutlass_library_objs.dir/cutlass_library_objs.unity.19976851ec6d.cu.o\u001b[0m\n",
            "[ 87%] Built target cutlass_library_objs\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX static library libcutlass.a\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX shared library libcutlass.so\u001b[0m\n",
            "[ 88%] Built target cutlass_lib\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/10_planar_complex/CMakeFiles/10_planar_complex.dir/planar_complex.cu.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CUDA object examples/11_planar_complex_array/CMakeFiles/11_planar_complex_array.dir/planar_complex_array.cu.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cutlass_profiler.cu.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/options.cu.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/performance_report.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/enumerated_types.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_allocation.cu.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gpu_timer.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/device_context.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cudnn_helpers.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/problem_space.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/cublas_helpers.cu.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 94%] Built target cutlass_library_static\n",
            "[ 94%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/rank_2k_operation_profiler.cu.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/trmm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/symm_operation_profiler.cu.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv2d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/conv3d_operation_profiler.cu.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CUDA object tools/profiler/CMakeFiles/cutlass_profiler.dir/src/sparse_gemm_operation_profiler.cu.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CUDA executable 11_planar_complex_array\u001b[0m\n",
            "[ 98%] Built target 11_planar_complex_array\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CUDA executable 10_planar_complex\u001b[0m\n",
            "[ 99%] Built target 10_planar_complex\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable cutlass_profiler\u001b[0m\n",
            "[100%] Built target cutlass_profiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torch-int/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33WhSKg5fAuf",
        "outputId": "060d7881-27f0-4c76-bba3-f840a6b91aba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torch-int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like someone here\n",
        "\n",
        "https://github.com/NVIDIA/FasterTransformer/issues/381\n",
        "\n",
        "is running into similar issues. After adding:\n",
        "\n",
        "\n",
        "```\n",
        "            include_dirs=['torch_int/kernels/include','submodules/cutlass/include'],\n",
        "```\n",
        "\n",
        "\n",
        "I am now missing cutlass/util/input something something. Luckily, using\n",
        "\n",
        "https://nvidia.github.io/cutlass/files.html\n",
        "\n",
        "I'm able to gain a foggy picture of what the installation was maybe(?) supposed to look like, and I found index_sequence.h inside submodules/cutlass/tools/util/include/cutlass/util/. Turns out that the setup include_dirs needs explicit paths, so I just have to go through and patch up these holesâ€”what I'm confused about is why I have to do this myself?\n",
        "\n",
        "HOLY CROW IT WORKED!\n",
        "```\n",
        "include_dirs=[\n",
        "    'torch_int/kernels/include',\n",
        "    'submodules/cutlass/include',\n",
        "    'submodules/cutlass/tools/util/include',\n",
        "    'submodules/cutlass/tools'\n",
        "]\n",
        "```\n",
        "This is what did it. Bless up\n"
      ],
      "metadata": {
        "id": "pVh9eM_ogT_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gonna try to use the pip as claude recommended\n",
        "\n",
        "! pip install -e .\n",
        "#! python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfpQCEdrKGf8",
        "outputId": "524372d9-82ea-4e60-e6ef-9a41df691fbc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/torch-int\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: torch-int\n",
            "  Running setup.py develop for torch-int\n",
            "Successfully installed torch-int-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import smoothquant\n",
        "import torch_int"
      ],
      "metadata": {
        "id": "FCA9Lpz6fi2u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there was no path for some reason\n",
        "import sys\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brBwvh8Qm4y_",
        "outputId": "6fdd2ff2-fa55-4219-e9be-36fe9a4e614d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.10/site-packages', '/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/smoothquant')\n",
        "sys.path.append('/content/torch-int')\n",
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flw9Z4eryC4n",
        "outputId": "650fcd5b-11e5-483f-af22-f47f2003c882"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.10/site-packages', '/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/smoothquant', '/content/torch-int']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the issue I'm facing is there not being recognition of smoothquant.opt."
      ],
      "metadata": {
        "id": "_YOs4muPm9Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://stackoverflow.com/questions/487971/is-there-a-standard-way-to-list-names-of-python-modules-in-a-package\n",
        "\n",
        "import imp\n",
        "import os\n",
        "MODULE_EXTENSIONS = ('.py', '.pyc', '.pyo')\n",
        "\n",
        "def package_contents(package_name):\n",
        "    file, pathname, description = imp.find_module(package_name)\n",
        "    if file:\n",
        "        raise ImportError('Not a package: %r', package_name)\n",
        "    # Use a set because some may be both source and compiled.\n",
        "    return set([os.path.splitext(module)[0]\n",
        "        for module in os.listdir(pathname)\n",
        "        if module.endswith(MODULE_EXTENSIONS)])"
      ],
      "metadata": {
        "id": "jgXJR1AzpTLR"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "package_contents('smoothquant')\n",
        "\n",
        "# it's here, so why isn't it being recognized...?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp5tkmLspdRY",
        "outputId": "0462a7c9-1828-42b3-d66a-d5713d903846"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__init__', 'calibration', 'fake_quant', 'opt', 'ppl_eval', 'smooth'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so now the smoothquant.smoothquant.opt somehow recognizes, but import torch_int doesn't work. Now, after 30 minutes, i realize that maybe I need to say torch-int. But sometimes it's torch_int, sometimes it's torch-int, so what am I supposed to do with that? torch-int is the outer, torch_int is the inner.\n",
        "\n",
        "Okay, now it worked. Not sure what changed, maybe it was using pip install instead of a manual run."
      ],
      "metadata": {
        "id": "a7WCF4AGwZUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2d738D2zHUz",
        "outputId": "9b83029f-c9d4-4505-ddf7-f6e64050647d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.36.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/site-packages (from transformers) (0.24.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
            "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.36.0\n",
            "    Uninstalling transformers-4.36.0:\n",
            "      Successfully uninstalled transformers-4.36.0\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.43.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/huggingface/transformers/issues/27701\n",
        "\n",
        "Now I'm getting AttributeError: type object 'OPTDecoder' has no attribute '_prepare_decoder_attention_mask'\n",
        "with the smoothquant import. This is because of a refactor that happened in November 2023, so I'm going to change opt.py's _prepare_decoder_attention_mask to _prepare_4d_causal_attention_mask. Line 375 in opt.py.\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py\n",
        "\n",
        "Above is also helpful for seeing the documentation of OPTDecoder and why I'm having this issue. I literally don't see _prepare_decoder_attention_mask or _prepare_4d_causal_attention_mask in the OPTDecoder class either, so I'm going to try and revert to the 4.26.0 release.\n",
        "\n",
        "Okay. Welp. That didn't work.\n"
      ],
      "metadata": {
        "id": "-FiAGqFhzmI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip cache purge\n",
        "!pip install transformers==4.26.0 --no-cache-dir\n",
        "\n",
        "#! pip install --force-reinstall transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2VNnI_2N3h",
        "outputId": "8707dcc1-8606-48c1-b4d8-2577f104c076"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.43.3\n",
            "Uninstalling transformers-4.43.3:\n",
            "  Successfully uninstalled transformers-4.43.3\n",
            "Files removed: 102\n",
            "Collecting transformers==4.26.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (0.24.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (2024.7.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers==4.26.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.26.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.26.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.26.0) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.26.0) (2024.7.4)\n",
            "Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m313.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m323.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, it kind of looks like that when I have transformers 4.43.3 (latest), there's no definition for this _prepare attribute, but when I had 4.26.0 there was one. Let me check that I'm not tripping out."
      ],
      "metadata": {
        "id": "FhrIxKks7QvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for some reason, smoothquant.smoothquant.opt works?\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# yet, when i print the version, it seems like transformers should be able to reference it.\n",
        "# ??\n",
        "\n",
        "import torch_int\n",
        "from smoothquant.smoothquant.opt import Int8OPTForCausalLM\n",
        "model = Int8OPTForCausalLM.from_pretrained(\"mit-han-lab/opt-30b-smoothquant\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "DKYIiWlbgfsq",
        "outputId": "7c67fe05-4e9b-444b-cfde-9f2dd9343883"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.26.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'OPTDecoder' has no attribute '_prepare_decoder_attention_mask'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-590a9fdca8ee>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothquant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInt8OPTForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mit-han-lab/opt-30b-smoothquant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/smoothquant/smoothquant/opt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInt8OPTDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPTPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[1;32m    321\u001b[0m     \u001b[0mTransformer\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0mconsisting\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mEach\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mInt8OPTDecoderLayer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/smoothquant/smoothquant/opt.py\u001b[0m in \u001b[0;36mInt8OPTDecoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0mget_input_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPTDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mset_input_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPTDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0m_prepare_decoder_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPTDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_decoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0mold_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPTDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'OPTDecoder' has no attribute '_prepare_decoder_attention_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5x-01auil_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}